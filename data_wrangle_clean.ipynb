{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling and cleaning:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Propublica API Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json; import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "pd.set_option('display.max_rows', None); pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def er(url, head): # error catch, recursive\n",
    "    global r; r = requests.get(url, headers = head)\n",
    "    if r.status_code != 200: er(url, head) \n",
    "    if r.text[11:13] != 'OK': er(url, head)\n",
    "    return r\n",
    "\n",
    "\n",
    "def st(state):\n",
    "    df = pd.DataFrame(columns=['st', 'dist', 'name', 'id', 'party', 'total', 'pac', 'win'])\n",
    "    u = 'https://api.propublica.org/campaign-finance/v1/2020'\n",
    "    h = {'X-API-Key': '1-800-API-KEYS-R-US'}\n",
    "    cans = er(u+'/races/%s/house.json' %state, h).json()['results'] #list of candidates who ran in each state\n",
    "    \n",
    "    for i in range(len(cans)):\n",
    "        df = df.append({'st': state,\n",
    "                        'name': cans[i]['candidate']['name'].lower(), \n",
    "                        'id': cans[i]['candidate']['id'], \n",
    "                        'party':cans[i]['candidate']['party'],},ignore_index = True)\n",
    "        \n",
    "    for j in range(len(df)):\n",
    "        can_data = er(u+'/candidates/%s.json' %(df.iloc[j,3]), h).json() # individual candidate info\n",
    "        district = can_data['results'][0]['district']\n",
    "        df.iat[j,1] = 98 if district == None else 99 if not district[16:18].isnumeric() else int(district[16:18])\n",
    "        df.iat[j,5] = int(can_data['results'][0]['total_contributions'])\n",
    "        df.iat[j,6] = int(can_data['results'][0]['total_from_pacs'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', \n",
    "    'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', \n",
    "    'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = st(states[0])\n",
    "#for i in range(1, len(states)): df1 = pd.concat([df1, st(states[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv('propub_csv_final', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Wikipedia Scrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbreviate(x): \n",
    "    abv = {'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO', \n",
    "     'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID', \n",
    "     'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', \n",
    "     'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "     'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', \n",
    "     'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', \n",
    "     'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC', \n",
    "     'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA', \n",
    "     'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'}\n",
    "    a = x.split(); b = len(a)\n",
    "    return abv[a[0]] if b == 2 else abv[' '.join([a[0], a[1]])]\n",
    "\n",
    "def spanish(x):\n",
    "    sp = {'á': 'a', 'é': 'e', 'í': 'i', 'ú': 'u'}\n",
    "    return ''.join(sp[i] if i in sp.keys() else i for i in list(x))\n",
    "        \n",
    "\n",
    "def wiki():\n",
    "    df=pd.DataFrame(columns=['st', 'dist', 'st/dist','name','party', 'id'])\n",
    "    url = 'https://en.wikipedia.org/wiki/List_of_current_members_of_the_United_States_House_of_Representatives'\n",
    "    s = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    rows = s.find('table',{'class': 'wikitable sortable', 'id':'votingmembers'}).tbody.find_all('tr')\n",
    "    \n",
    "    for i in range(1, len(rows)):\n",
    "        tds = rows[i].find_all('td')\n",
    "        if len(tds) == 9:\n",
    "            values = [tds[0].text.replace('\\n', ''), spanish(tds[1].text.replace('\\n', '').lower()), \n",
    "                      tds[3].text.replace('\\n', '')]\n",
    "        else: values = [tds[0].text.replace('\\n', ''), 'vacant', 'vacant']\n",
    "        df = df.append(pd.Series(values, index=['st/dist','name','party']), ignore_index=True)\n",
    "        \n",
    "        df['dist'] = df['st/dist'].map(lambda x: int(x.split()[-1]) if x.split()[-1].isnumeric() else 98) #dist\n",
    "        df['st'] = df['st/dist'].map(abbreviate)\n",
    "        df['party'] = df['party'].map(lambda x: x[0:3].upper())\n",
    "\n",
    "    return df\n",
    "\n",
    "def url_nm(name): return ''.join([i +'+' for i in name.split()]) #create FEC url search name string\n",
    "\n",
    "def ids(x): # FEC scrape to get id's for each candidate\n",
    "    result = BeautifulSoup(requests.get('https://www.fec.gov/search/?query=%s&type=candidates' %x).text,\n",
    "                         'html.parser').find_all(class_='post__path t-small t-sans')\n",
    "    return result[0].text.replace('<div>', '').split('/')[3] if result !=[] else 'no result'\n",
    "\n",
    "def wiki_id(x): #iterate over df apply FEC scrape\n",
    "    for i in range(len(x)):\n",
    "        x.iat[i,5] = ids(url_nm(x.iat[i,3]))\n",
    "    return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wk1 = wiki_id(wiki())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wk1.to_csv('wiki_fec_csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pd.read_csv('/.../Desktop/propub_csv_final')\n",
    "wk = pd.read_csv('/.../Desktop/wiki_fec_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winners FEC ID check\n",
    "for i in range(len(pp)):\n",
    "    match = wk[(wk['st']==pp.iloc[i,0]) & (wk['dist']==pp.iloc[i,1])]\n",
    "    if match.empty: pp.iat[i, 7] = False\n",
    "    elif match['name'].values == 'vacant': pp.iat[i, 7] = False\n",
    "    else: \n",
    "        pp.iat[i, 7]= ((pp.iloc[i,2].split(',')[0] == match['name'].values[0].split(' ')[-1]) \n",
    "        or (pp.iloc[i,3] == match.id.values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia says there are 435 members of congress. How many dow we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n"
     ]
    }
   ],
   "source": [
    "print(len(pp[pp['win']==1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have too many, something is off. Maybe there are duplicate winners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_dup = pp[pp['win']==1.0].groupby(['st', 'dist'], as_index = False).agg({'win':['count']})\n",
    "pp_dup = pp_dup[pp_dup['win']['count']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     st dist   win\n",
      "             count\n",
      "45   CA   25     2\n",
      "56   CA   36     2\n",
      "62   CA   42     2\n",
      "70   CA   50     2\n",
      "130  IA    2     2\n",
      "175  LA    5     2\n",
      "191  MD    6     2\n",
      "192  MD    7     2\n",
      "266  NV    3     2\n",
      "331  PA   12     2\n"
     ]
    }
   ],
   "source": [
    "print(pp_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are duplicates. Let's print out the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     st  dist             name         id party    total     pac  win\n",
      "270  CA    25     garcia, mike  H0CA25162   REP   358113    8893  1.0\n",
      "369  CA    25  garcia, michael  H0CA25105   REP  9348402  897272  1.0\n",
      "     st  dist            name         id party    total     pac  win\n",
      "173  CA    36      ruiz, raul  H0CA36177   REP    12039       0  1.0\n",
      "504  CA    36  ruiz, raul dr.  H2CA36439   DEM  2428672  968325  1.0\n",
      "     st  dist          name         id party    total     pac  win\n",
      "180  CA    42  calvert, ken  H0CA42209   REP        0       0  1.0\n",
      "505  CA    42  calvert, ken  H2CA37023   REP  1386747  611550  1.0\n",
      "     st  dist           name         id party     total     pac  win\n",
      "278  CA    50  issa, darrell  H0CA50178   REP  10196220  414060  1.0\n",
      "300  CA    50  issa, darrell  H0CA48024   REP         0       0  1.0\n",
      "      st  dist                            name         id party    total     pac  win\n",
      "1254  IA     2  miller-meeks, mariannette jane  H0IA02180   REP   748422  110770  1.0\n",
      "1264  IA     2  miller-meeks, mariannette jane  H8IA02043   REP  1585688  367312  1.0\n",
      "      st  dist                 name         id party    total     pac  win\n",
      "1365  LA     5        letlow, julia  H2LA05126   REP        0       0  1.0\n",
      "1384  LA     5  letlow, luke joshua  H0LA05120   REP  1395022  190150  1.0\n",
      "      st  dist          name         id party    total  pac  win\n",
      "1406  MD     6  trone, david  H8MD06168   DEM  2703087    0  1.0\n",
      "1437  MD     6  trone, david  H6MD08549   DEM  2704019    0  1.0\n",
      "      st  dist           name         id party    total     pac  win\n",
      "1411  MD     7  mfume, kweisi  H0MD07114   DEM   457321   78495  1.0\n",
      "1493  MD     7  mfume, kweisi  H6MD07020   DEM  1006454  368319  1.0\n",
      "      st  dist        name         id party    total      pac  win\n",
      "1872  NV     3  lee, susie  H8NV03200   DEM        0        0  1.0\n",
      "1904  NV     3  lee, susie  H6NV04020   DEM  4137510  1286408  1.0\n",
      "      st  dist          name         id party    total     pac  win\n",
      "2663  PA    12  keller, fred  H0PA12181   REP  1512978  445712  1.0\n",
      "2673  PA    12  keller, fred  H9PA12018   REP        0       0  1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pp_dup)):\n",
    "    print(pp[(pp['st']==ppd.iloc[i,0]) & (pp['dist']==ppd.iloc[i,1]) & (pp['win']==1.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging into them:\n",
    "\n",
    "Duplicates with $0 donations:\n",
    "\n",
    "    2673  PA    12  keller, fred  H9PA12018\n",
    "    1872  NV     3  lee, susie  H8NV03200 \n",
    "    300  CA    50  issa, darrell  H0CA48024\n",
    "    180  CA    42  calvert, ken  H0CA42209\n",
    "\n",
    "Opposing candidate with nearly identical name:\n",
    "\n",
    "    173  CA    36      ruiz, raul  H0CA36177\n",
    "    \n",
    "Letlow:\n",
    "\n",
    "    1365  LA     5        letlow, julia  H2LA05126\n",
    "    \n",
    "No record found on FEC website for:\n",
    "\n",
    "    1411  MD     7  mfume, kweisi  H0MD07114\n",
    "    1406  MD     6  trone, david  H8MD06168\n",
    "    1035  IA     2  miller-meeks, mariannette jane  H0IA02180\n",
    "    289  CA    25     garcia, mike  H0CA25162\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "438 minus 10 duplicates = 428 members. Now we are short.\n",
    "\n",
    "\n",
    "Is there a winner for every district listed on Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk['match'] = wk.apply(lambda i: \n",
    "                         not (pp[(pp['st']==i.st) & (pp['dist']==i.dist) & (pp['win'] == 1.0)]).empty, axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     st  dist       st/dist              name party         id  match\n",
      "106  FL    20    Florida 20            vacant   VAC  no result  False\n",
      "174  LA     2   Louisiana 2            vacant   VAC  no result  False\n",
      "254  NM     1  New Mexico 1            vacant   VAC  no result  False\n",
      "308  OH    11       Ohio 11            vacant   VAC  no result  False\n",
      "366  TX     6       Texas 6            vacant   VAC  no result  False\n",
      "384  TX    24      Texas 24    beth van duyne   REP  no result  False\n",
      "394  TX    34      Texas 34  filemon vela jr.   DEM  no result  False\n"
     ]
    }
   ],
   "source": [
    "print(wk[wk['match']==False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "438 minus 10 duplicates + 5 vacancies + 2 missing = 435"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#propub_winners_csv = pp.sort_values((['st', 'dist']), ascending=True)\n",
    "#propub_winners_csv.to_csv('propub_winners_csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pd.read_csv('/.../Desktop/propub_winners_csv_edit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n"
     ]
    }
   ],
   "source": [
    "print(len(win[win['win']==1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
